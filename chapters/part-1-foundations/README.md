---
layout: default
title: Part I - Foundations of Learning and Complexity
nav_order: 3
has_children: true
---

# Part I: Foundations of Learning and Complexity

> *Understanding the theoretical bedrock of machine learning*

---

## Overview

Part I establishes the theoretical foundations that underpin all of deep learning. Before diving into neural network architectures, we must understand **why** certain approaches work—and the answer lies in information theory, complexity, and compression.

## Chapters

| # | Chapter | Key Concept |
|---|---------|-------------|
| 1 | [The Minimum Description Length Principle](./01-minimum-description-length.md) | Best model = shortest description |
| 2 | [Kolmogorov Complexity](./02-kolmogorov-complexity.md) | Complexity = shortest program length |
| 3 | [Keeping Neural Networks Simple](./03-keeping-nn-simple.md) | Training = compression |
| 4 | [The Coffee Automaton](./04-coffee-automaton.md) | Complexity rises then falls |
| 5 | [The First Law of Complexodynamics](./05-complexodynamics.md) | Interestingness is inevitable |

## The Big Picture

```
MDL + Kolmogorov → Why simplicity matters
         ↓
Hinton's Paper → How this applies to neural nets
         ↓
Coffee Automaton + Complexodynamics → Why learning is even possible
```

## Key Takeaway

> **Learning is compression. Intelligence finds patterns. The universe creates interesting structures on its way from order to chaos—and neural networks exploit this fact.**

## Prerequisites

- Basic probability and statistics
- Familiarity with information theory concepts (bits, entropy)
- Curiosity about "why things work"

## What You'll Be Able To Do After Part I

- Explain why regularization works from first principles
- Understand the deep connection between compression and learning
- Appreciate why neural networks can generalize
- Think about AI from a theoretical perspective

