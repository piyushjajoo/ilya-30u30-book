# üìö The Ilya 30u30 Deep Learning Compendium

> A comprehensive book based on the 30 papers and resources recommended by Ilya Sutskever for mastering Artificial Intelligence.

## About This Book

This book transforms Ilya Sutskever's legendary "30u30" reading list into an accessible, structured learning journey. Each chapter distills complex research papers into clear explanations with visual Mermaid diagrams.

## Source

Based on [Ilya's 30u30 Reading List](https://github.com/jayxin/Ilya-30u30)

---

## üìñ Table of Contents

# Part I: Foundations of Learning and Complexity
*Understanding the theoretical bedrock of machine learning*

| Ch | Title | Paper/Source |
|----|-------|--------------|
| 1 | [The Minimum Description Length Principle](./chapters/part-1-foundations/01-minimum-description-length.md) | [Gr√ºnwald, 2004](https://arxiv.org/abs/math/0406077) |
| 2 | [Kolmogorov Complexity and Algorithmic Randomness](./chapters/part-1-foundations/02-kolmogorov-complexity.md) | [Shen et al.](https://bookstore.ams.org/surv-220/) |
| 3 | [Keeping Neural Networks Simple](./chapters/part-1-foundations/03-keeping-nn-simple.md) | [Hinton & Van Camp, 1993](https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html) |
| 4 | [The Coffee Automaton](./chapters/part-1-foundations/04-coffee-automaton.md) | [Aaronson et al., 2014](https://arxiv.org/abs/1405.6903) |
| 5 | [The First Law of Complexodynamics](./chapters/part-1-foundations/05-complexodynamics.md) | [Aaronson Blog](https://scottaaronson.blog/?p=762) |

---

# Part II: Convolutional Neural Networks
*The revolution in visual understanding*

| Ch | Title | Paper/Source |
|----|-------|--------------|
| 6 | [AlexNet - The ImageNet Breakthrough](./chapters/part-2-cnns/06-alexnet.md) | [Krizhevsky et al., 2012](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) |
| 7 | [CS231n - CNNs for Visual Recognition](./chapters/part-2-cnns/07-cs231n.md) | [Stanford Course](https://cs231n.stanford.edu/) |
| 8 | [Deep Residual Learning (ResNet)](./chapters/part-2-cnns/08-resnet.md) | [He et al., 2015](https://arxiv.org/abs/1512.03385) |
| 9 | [Identity Mappings in Deep Residual Networks](./chapters/part-2-cnns/09-identity-mappings.md) | [He et al., 2016](https://arxiv.org/abs/1603.05027) |
| 10 | [Dilated Convolutions for Multi-Scale Context](./chapters/part-2-cnns/10-dilated-convolutions.md) | [Yu & Koltun, 2015](https://arxiv.org/abs/1511.07122) |

---

# Part III: Sequence Models and Recurrent Networks
*Learning from sequential data*

| Ch | Title | Paper/Source |
|----|-------|--------------|
| 11 | [The Unreasonable Effectiveness of RNNs](./chapters/part-3-rnns/11-rnn-effectiveness.md) | [Karpathy Blog, 2015](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) |
| 12 | [Understanding LSTM Networks](./chapters/part-3-rnns/12-lstm-networks.md) | [Colah's Blog, 2015](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) |
| 13 | [Recurrent Neural Network Regularization](./chapters/part-3-rnns/13-rnn-regularization.md) | [Zaremba et al., 2014](https://arxiv.org/abs/1409.2329) |
| 14 | [Relational Recurrent Neural Networks](./chapters/part-3-rnns/14-relational-rnns.md) | [Santoro et al., 2018](https://arxiv.org/abs/1806.01822) |

---

# Part IV: Attention and Transformers
*The attention revolution*

| Ch | Title | Paper/Source |
|----|-------|--------------|
| 15 | [Neural Machine Translation with Attention](./chapters/part-4-attention/15-nmt-attention.md) | [Bahdanau et al., 2014](https://arxiv.org/abs/1409.3215) |
| 16 | [Attention Is All You Need (Transformers)](./chapters/part-4-attention/16-transformers.md) | [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762) |
| 17 | [The Annotated Transformer](./chapters/part-4-attention/17-annotated-transformer.md) | [Harvard NLP](http://nlp.seas.harvard.edu/annotated-transformer/) |

---

# Part V: Advanced Architectures
*Specialized neural network designs*

| Ch | Title | Paper/Source |
|----|-------|--------------|
| 18 | [Pointer Networks](./chapters/part-5-advanced/18-pointer-networks.md) | [Vinyals et al., 2015](https://arxiv.org/abs/1506.03134) |
| 19 | [Order Matters: Sequence to Sequence for Sets](./chapters/part-5-advanced/19-seq2seq-sets.md) | [Vinyals et al., 2015](https://arxiv.org/abs/1511.06391) |
| 20 | [Neural Turing Machines](./chapters/part-5-advanced/20-neural-turing-machines.md) | [Graves et al., 2014](https://arxiv.org/abs/1410.5401) |
| 21 | [Neural Message Passing for Quantum Chemistry](./chapters/part-5-advanced/21-message-passing.md) | [Gilmer et al., 2017](https://arxiv.org/abs/1704.01212) |
| 22 | [A Simple Neural Network Module for Relational Reasoning](./chapters/part-5-advanced/22-relational-reasoning.md) | [Santoro et al., 2017](https://arxiv.org/abs/1706.01427) |
| 23 | [Variational Lossy Autoencoder](./chapters/part-5-advanced/23-vlae.md) | [Chen et al., 2016](https://arxiv.org/abs/1611.02731) |

---

# Part VI: Scaling and Efficiency
*Training neural networks at scale*

| Ch | Title | Paper/Source |
|----|-------|--------------|
| 24 | [Deep Speech 2](./chapters/part-6-scaling/24-deep-speech-2.md) | [Amodei et al., 2015](https://arxiv.org/abs/1512.02595) |
| 25 | [Scaling Laws for Neural Language Models](./chapters/part-6-scaling/25-scaling-laws.md) | [Kaplan et al., 2020](https://arxiv.org/abs/2001.08361) |
| 26 | [GPipe - Pipeline Parallelism](./chapters/part-6-scaling/26-gpipe.md) | [Huang et al., 2018](https://arxiv.org/abs/1811.06965) |

---

# Part VII: The Future of Intelligence
*Philosophical and theoretical perspectives*

| Ch | Title | Paper/Source |
|----|-------|--------------|
| 27 | [Machine Super Intelligence](./chapters/part-7-future/27-future.md) | [Shane Legg, 2008](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf) |

---

## How to Read This Book

Each chapter includes:
- üìñ **Accessible explanations** - Complex concepts made simple
- üìä **Mermaid diagrams** - Visual representations of key ideas
- üî¢ **Key equations** - Essential formulas with intuitive explanations
- üîó **Connections** - Links between related papers and concepts
- üí° **Modern applications** - How ideas are used today
- üìö **References** - Original papers and further reading

### Suggested Reading Paths

#### üéØ Standard Path (Recommended)
Read chapters 1-27 in order for a complete journey from theory to practice.

#### ‚ö° Practitioner's Path
If you want to build things quickly:
1. Chapter 6 (AlexNet) ‚Üí Chapter 8 (ResNet)
2. Chapter 11-12 (RNNs/LSTMs)
3. Chapter 15-17 (Attention/Transformers)
4. Chapter 25 (Scaling Laws)

#### üß† Theorist's Path
If you love theory and foundations:
1. Chapters 1-5 (Full Part I)
2. Chapter 27 (Superintelligence)
3. Then practical chapters as needed

#### üî¨ Researcher's Path
For cutting-edge architectures:
1. Chapters 16-17 (Transformers)
2. Chapters 18-23 (Advanced Architectures)
3. Chapters 25-26 (Scaling)

---

## Support This Project

If you find this book useful, please consider ‚≠ê starring the [GitHub repository](https://github.com/piyushjajoo/ilya-30u30-book) to help others discover it!

---

## License

Educational content based on public research papers. All original papers are cited with links to their sources.
